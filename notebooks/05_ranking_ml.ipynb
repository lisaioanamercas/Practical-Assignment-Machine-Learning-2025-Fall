{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Product Ranking with ML Algorithms (From Scratch)\n",
                "\n",
                "## Objective\n",
                "Build a ranking system for product upselling using:\n",
                "1. **Naive Bayes** (implemented from scratch)\n",
                "2. **k-NN** (implemented from scratch)\n",
                "3. **Baseline comparisons** (popularity, revenue)\n",
                "\n",
                "### Scoring Formula\n",
                "$$\\text{Score}(p | \\text{cart}) = P(p | \\text{cart}) \\times \\text{price}(p)$$\n",
                "\n",
                "This maximizes expected revenue."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "import sys\n",
                "from collections import defaultdict\n",
                "\n",
                "# Add src to path\n",
                "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
                "\n",
                "from data_loader import load_raw_data, SAUCES\n",
                "from preprocessing import create_receipt_features, normalize_features\n",
                "from models.ranking import (\n",
                "    NaiveBayesClassifier, KNNClassifier, ProductRanker,\n",
                "    calculate_hit_at_k, calculate_precision_at_k, calculate_mrr\n",
                ")\n",
                "\n",
                "# Settings\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load and Prepare Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data\n",
                "DATA_PATH = Path('../data/raw/ap_dataset.csv')\n",
                "df = load_raw_data(DATA_PATH)\n",
                "\n",
                "print(f\"Dataset: {len(df)} rows, {df['id_bon'].nunique()} receipts\")\n",
                "\n",
                "# Define candidate products for upselling\n",
                "# (drinks + sauces + sides)\n",
                "DRINKS = [p for p in df['retail_product_name'].unique() \n",
                "          if any(x in p.lower() for x in ['pepsi', 'cola', 'aqua', '7up', 'lipton', 'mirinda', 'dew', 'prigat'])]\n",
                "SIDES = [p for p in df['retail_product_name'].unique() \n",
                "         if any(x in p.lower() for x in ['fries', 'potatoes', 'baked'])]\n",
                "\n",
                "CANDIDATE_PRODUCTS = SAUCES + DRINKS + SIDES\n",
                "print(f\"\\nCandidate products for ranking: {len(CANDIDATE_PRODUCTS)}\")\n",
                "for p in CANDIDATE_PRODUCTS:\n",
                "    print(f\"  - {p}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get product prices\n",
                "product_prices = df.groupby('retail_product_name')['SalePriceWithVAT'].first().to_dict()\n",
                "\n",
                "# Get product popularity (for baseline)\n",
                "total_receipts = df['id_bon'].nunique()\n",
                "product_popularity = {}\n",
                "for product in df['retail_product_name'].unique():\n",
                "    count = df[df['retail_product_name'] == product]['id_bon'].nunique()\n",
                "    product_popularity[product] = count / total_receipts\n",
                "\n",
                "print(\"Product prices and popularity loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Create Features at Receipt Level"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create receipt-level features\n",
                "# Exclude candidate products from features (we're predicting them)\n",
                "features_df = create_receipt_features(df, use_binary=True, exclude_products=CANDIDATE_PRODUCTS)\n",
                "\n",
                "print(f\"Created features for {len(features_df)} receipts\")\n",
                "print(f\"Number of features: {len(features_df.columns) - 1}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create target labels for each candidate product\n",
                "y_dict = {}\n",
                "for product in CANDIDATE_PRODUCTS:\n",
                "    receipts_with_product = df[df['retail_product_name'] == product]['id_bon'].unique()\n",
                "    y_dict[product] = features_df['id_bon'].isin(receipts_with_product).astype(int).values\n",
                "    \n",
                "    positive_rate = y_dict[product].mean()\n",
                "    print(f\"{product}: {y_dict[product].sum()} positive ({positive_rate:.1%})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare feature matrix\n",
                "feature_cols = [col for col in features_df.columns if col != 'id_bon']\n",
                "X = features_df[feature_cols].values\n",
                "receipt_ids = features_df['id_bon'].values\n",
                "\n",
                "print(f\"Feature matrix shape: {X.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Train/Test Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Split indices\n",
                "indices = np.arange(len(X))\n",
                "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
                "\n",
                "X_train = X[train_idx]\n",
                "X_test = X[test_idx]\n",
                "\n",
                "# Split y_dict too\n",
                "y_train_dict = {p: y[train_idx] for p, y in y_dict.items()}\n",
                "y_test_dict = {p: y[test_idx] for p, y in y_dict.items()}\n",
                "\n",
                "# Normalize\n",
                "X_train_norm, mean, std = normalize_features(X_train)\n",
                "X_test_norm, _, _ = normalize_features(X_test, mean=mean, std=std)\n",
                "\n",
                "print(f\"Training set: {len(X_train)} receipts\")\n",
                "print(f\"Test set: {len(X_test)} receipts\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train Ranking Models\n",
                "\n",
                "### 4.1 Naive Bayes (From Scratch)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Naive Bayes ranker\n",
                "nb_ranker = ProductRanker(algorithm='naive_bayes')\n",
                "nb_ranker.fit(X_train_norm, y_train_dict, feature_cols, product_prices)\n",
                "\n",
                "print(f\"Trained Naive Bayes models for {len(nb_ranker.models)} products\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 k-NN (From Scratch)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train k-NN ranker (use subset for speed)\n",
                "# k-NN is slow, so we use a smaller training set\n",
                "knn_sample_size = min(500, len(X_train_norm))\n",
                "knn_sample_idx = np.random.choice(len(X_train_norm), knn_sample_size, replace=False)\n",
                "\n",
                "X_train_knn = X_train_norm[knn_sample_idx]\n",
                "y_train_knn_dict = {p: y[knn_sample_idx] for p, y in y_train_dict.items()}\n",
                "\n",
                "knn_ranker = ProductRanker(algorithm='knn')\n",
                "knn_ranker.fit(X_train_knn, y_train_knn_dict, feature_cols, product_prices)\n",
                "\n",
                "print(f\"Trained k-NN models for {len(knn_ranker.models)} products (on {knn_sample_size} samples)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Evaluation Setup\n",
                "\n",
                "For each receipt in test set:\n",
                "1. Remove one product from the cart\n",
                "2. Use ranking to predict what product to recommend\n",
                "3. Check if removed product is in Top-K"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_ranking(df, test_receipt_ids, ranker, X_test, candidate_products, k_values=[1, 3, 5]):\n",
                "    \"\"\"\n",
                "    Evaluate ranking performance by leave-one-out.\n",
                "    \n",
                "    For each test receipt:\n",
                "    1. Find candidate products that were actually purchased\n",
                "    2. Remove one product\n",
                "    3. Get ranking from model\n",
                "    4. Check if removed product is in Top-K\n",
                "    \"\"\"\n",
                "    results = {k: {'hits': 0, 'total': 0} for k in k_values}\n",
                "    mrr_scores = []\n",
                "    \n",
                "    for i, receipt_id in enumerate(test_receipt_ids):\n",
                "        # Get products in this receipt that are candidates\n",
                "        receipt_products = df[df['id_bon'] == receipt_id]['retail_product_name'].unique()\n",
                "        candidate_in_receipt = [p for p in receipt_products if p in candidate_products]\n",
                "        \n",
                "        if len(candidate_in_receipt) < 1:\n",
                "            continue\n",
                "        \n",
                "        # For each candidate product, treat it as the \"removed\" product\n",
                "        for removed_product in candidate_in_receipt:\n",
                "            # Get ranking (excluding the removed product from exclusions \n",
                "            # since we want to see if model predicts it)\n",
                "            other_candidates = [p for p in candidate_in_receipt if p != removed_product]\n",
                "            \n",
                "            # Get ranking for this receipt\n",
                "            rankings = ranker.rank(\n",
                "                X_test[i:i+1], \n",
                "                exclude_products=[other_candidates],  # Already bought\n",
                "                top_k=max(k_values)\n",
                "            )\n",
                "            \n",
                "            if len(rankings) == 0 or len(rankings[0]) == 0:\n",
                "                continue\n",
                "            \n",
                "            ranking = rankings[0]\n",
                "            ranked_products = [r[0] for r in ranking]\n",
                "            \n",
                "            # Check Hit@K\n",
                "            for k in k_values:\n",
                "                results[k]['total'] += 1\n",
                "                if removed_product in ranked_products[:k]:\n",
                "                    results[k]['hits'] += 1\n",
                "            \n",
                "            # Calculate MRR\n",
                "            if removed_product in ranked_products:\n",
                "                rank = ranked_products.index(removed_product) + 1\n",
                "                mrr_scores.append(1 / rank)\n",
                "            else:\n",
                "                mrr_scores.append(0)\n",
                "    \n",
                "    # Calculate metrics\n",
                "    hit_at_k = {k: results[k]['hits'] / results[k]['total'] if results[k]['total'] > 0 else 0 \n",
                "                for k in k_values}\n",
                "    mrr = np.mean(mrr_scores) if mrr_scores else 0\n",
                "    \n",
                "    return hit_at_k, mrr, results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get test receipt IDs\n",
                "test_receipt_ids = receipt_ids[test_idx]\n",
                "\n",
                "# Limit evaluation for speed (sample 200 test receipts)\n",
                "eval_sample_size = min(200, len(test_idx))\n",
                "eval_idx = np.random.choice(len(test_idx), eval_sample_size, replace=False)\n",
                "X_test_eval = X_test_norm[eval_idx]\n",
                "test_receipt_ids_eval = test_receipt_ids[eval_idx]\n",
                "\n",
                "print(f\"Evaluating on {eval_sample_size} test receipts...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Baseline: Popularity Ranking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PopularityBaseline:\n",
                "    \"\"\"Baseline that ranks by popularity.\"\"\"\n",
                "    \n",
                "    def __init__(self, popularity_dict, products):\n",
                "        self.popularity = popularity_dict\n",
                "        self.products = products\n",
                "        # Pre-compute ranking\n",
                "        self.ranking = sorted(products, key=lambda p: popularity_dict.get(p, 0), reverse=True)\n",
                "    \n",
                "    def rank(self, X, exclude_products=None, top_k=5):\n",
                "        if exclude_products is None:\n",
                "            exclude_products = [[] for _ in range(len(X))]\n",
                "        \n",
                "        rankings = []\n",
                "        for i, x in enumerate(X):\n",
                "            excluded = exclude_products[i] if i < len(exclude_products) else []\n",
                "            ranking = [(p, self.popularity.get(p, 0), self.popularity.get(p, 0)) \n",
                "                      for p in self.ranking if p not in excluded][:top_k]\n",
                "            rankings.append(ranking)\n",
                "        return rankings\n",
                "\n",
                "popularity_baseline = PopularityBaseline(product_popularity, CANDIDATE_PRODUCTS)\n",
                "print(\"Popularity Baseline Top 5:\")\n",
                "for p, pop, _ in popularity_baseline.rank(np.zeros((1, X.shape[1])))[0][:5]:\n",
                "    print(f\"  {p}: {pop:.2%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RevenueBaseline:\n",
                "    \"\"\"Baseline that ranks by expected revenue (popularity * price).\"\"\"\n",
                "    \n",
                "    def __init__(self, popularity_dict, price_dict, products):\n",
                "        self.popularity = popularity_dict\n",
                "        self.prices = price_dict\n",
                "        self.products = products\n",
                "        # Pre-compute expected value and ranking\n",
                "        self.ev = {p: popularity_dict.get(p, 0) * price_dict.get(p, 0) for p in products}\n",
                "        self.ranking = sorted(products, key=lambda p: self.ev.get(p, 0), reverse=True)\n",
                "    \n",
                "    def rank(self, X, exclude_products=None, top_k=5):\n",
                "        if exclude_products is None:\n",
                "            exclude_products = [[] for _ in range(len(X))]\n",
                "        \n",
                "        rankings = []\n",
                "        for i, x in enumerate(X):\n",
                "            excluded = exclude_products[i] if i < len(exclude_products) else []\n",
                "            ranking = [(p, self.ev.get(p, 0), self.popularity.get(p, 0)) \n",
                "                      for p in self.ranking if p not in excluded][:top_k]\n",
                "            rankings.append(ranking)\n",
                "        return rankings\n",
                "\n",
                "revenue_baseline = RevenueBaseline(product_popularity, product_prices, CANDIDATE_PRODUCTS)\n",
                "print(\"Revenue Baseline Top 5:\")\n",
                "for p, ev, _ in revenue_baseline.rank(np.zeros((1, X.shape[1])))[0][:5]:\n",
                "    print(f\"  {p}: EV={ev:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Run Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "k_values = [1, 3, 5]\n",
                "\n",
                "print(\"Evaluating Naive Bayes...\")\n",
                "nb_hit, nb_mrr, nb_results = evaluate_ranking(\n",
                "    df, test_receipt_ids_eval, nb_ranker, X_test_eval, CANDIDATE_PRODUCTS, k_values\n",
                ")\n",
                "print(f\"  Hit@1: {nb_hit[1]:.4f}, Hit@3: {nb_hit[3]:.4f}, Hit@5: {nb_hit[5]:.4f}, MRR: {nb_mrr:.4f}\")\n",
                "\n",
                "print(\"\\nEvaluating k-NN...\")\n",
                "# k-NN evaluation (sample for speed)\n",
                "knn_eval_size = min(50, len(X_test_eval))\n",
                "knn_hit, knn_mrr, knn_results = evaluate_ranking(\n",
                "    df, test_receipt_ids_eval[:knn_eval_size], knn_ranker, X_test_eval[:knn_eval_size], \n",
                "    CANDIDATE_PRODUCTS, k_values\n",
                ")\n",
                "print(f\"  Hit@1: {knn_hit[1]:.4f}, Hit@3: {knn_hit[3]:.4f}, Hit@5: {knn_hit[5]:.4f}, MRR: {knn_mrr:.4f}\")\n",
                "\n",
                "print(\"\\nEvaluating Popularity Baseline...\")\n",
                "pop_hit, pop_mrr, pop_results = evaluate_ranking(\n",
                "    df, test_receipt_ids_eval, popularity_baseline, X_test_eval, CANDIDATE_PRODUCTS, k_values\n",
                ")\n",
                "print(f\"  Hit@1: {pop_hit[1]:.4f}, Hit@3: {pop_hit[3]:.4f}, Hit@5: {pop_hit[5]:.4f}, MRR: {pop_mrr:.4f}\")\n",
                "\n",
                "print(\"\\nEvaluating Revenue Baseline...\")\n",
                "rev_hit, rev_mrr, rev_results = evaluate_ranking(\n",
                "    df, test_receipt_ids_eval, revenue_baseline, X_test_eval, CANDIDATE_PRODUCTS, k_values\n",
                ")\n",
                "print(f\"  Hit@1: {rev_hit[1]:.4f}, Hit@3: {rev_hit[3]:.4f}, Hit@5: {rev_hit[5]:.4f}, MRR: {rev_mrr:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comparison table\n",
                "comparison_data = [\n",
                "    {'Algorithm': 'Naive Bayes (scratch)', 'Hit@1': nb_hit[1], 'Hit@3': nb_hit[3], 'Hit@5': nb_hit[5], 'MRR': nb_mrr},\n",
                "    {'Algorithm': 'k-NN (scratch)', 'Hit@1': knn_hit[1], 'Hit@3': knn_hit[3], 'Hit@5': knn_hit[5], 'MRR': knn_mrr},\n",
                "    {'Algorithm': 'Popularity Baseline', 'Hit@1': pop_hit[1], 'Hit@3': pop_hit[3], 'Hit@5': pop_hit[5], 'MRR': pop_mrr},\n",
                "    {'Algorithm': 'Revenue Baseline', 'Hit@1': rev_hit[1], 'Hit@3': rev_hit[3], 'Hit@5': rev_hit[5], 'MRR': rev_mrr},\n",
                "]\n",
                "\n",
                "comparison_df = pd.DataFrame(comparison_data)\n",
                "comparison_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Bar chart comparison\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Hit@K comparison\n",
                "x = np.arange(len(comparison_df))\n",
                "width = 0.25\n",
                "\n",
                "ax = axes[0]\n",
                "ax.bar(x - width, comparison_df['Hit@1'], width, label='Hit@1', color='steelblue')\n",
                "ax.bar(x, comparison_df['Hit@3'], width, label='Hit@3', color='seagreen')\n",
                "ax.bar(x + width, comparison_df['Hit@5'], width, label='Hit@5', color='coral')\n",
                "ax.set_xlabel('Algorithm')\n",
                "ax.set_ylabel('Hit Rate')\n",
                "ax.set_title('Hit@K Comparison')\n",
                "ax.set_xticks(x)\n",
                "ax.set_xticklabels(comparison_df['Algorithm'], rotation=45, ha='right')\n",
                "ax.legend()\n",
                "ax.grid(True, alpha=0.3)\n",
                "\n",
                "# MRR comparison\n",
                "ax = axes[1]\n",
                "colors = ['steelblue', 'seagreen', 'coral', 'mediumpurple']\n",
                "ax.bar(comparison_df['Algorithm'], comparison_df['MRR'], color=colors)\n",
                "ax.set_xlabel('Algorithm')\n",
                "ax.set_ylabel('Mean Reciprocal Rank')\n",
                "ax.set_title('MRR Comparison')\n",
                "plt.xticks(rotation=45, ha='right')\n",
                "ax.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../results/ranking_comparison.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Example Recommendations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show example recommendations for a few test receipts\n",
                "print(\"Example Recommendations:\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "for i in range(min(5, len(X_test_eval))):\n",
                "    receipt_id = test_receipt_ids_eval[i]\n",
                "    actual_products = df[df['id_bon'] == receipt_id]['retail_product_name'].unique()\n",
                "    \n",
                "    # Get rankings\n",
                "    nb_ranking = nb_ranker.rank(X_test_eval[i:i+1], top_k=3)[0]\n",
                "    \n",
                "    print(f\"\\nReceipt {receipt_id}:\")\n",
                "    print(f\"  Actual products: {list(actual_products)[:5]}...\")\n",
                "    print(f\"  Naive Bayes Top 3:\")\n",
                "    for p, score, prob in nb_ranking:\n",
                "        marker = \"✓\" if p in actual_products else \"✗\"\n",
                "        print(f\"    {marker} {p}: Score={score:.2f}, P={prob:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 70)\n",
                "print(\"RANKING SYSTEM SUMMARY\")\n",
                "print(\"=\" * 70)\n",
                "print(f\"\\nCandidate products: {len(CANDIDATE_PRODUCTS)}\")\n",
                "print(f\"Training samples: {len(X_train)}\")\n",
                "print(f\"Test samples evaluated: {eval_sample_size}\")\n",
                "\n",
                "print(\"\\n\" + \"-\" * 70)\n",
                "print(\"PERFORMANCE COMPARISON\")\n",
                "print(\"-\" * 70)\n",
                "print(f\"{'Algorithm':<25} {'Hit@1':<10} {'Hit@3':<10} {'Hit@5':<10} {'MRR':<10}\")\n",
                "print(\"-\" * 70)\n",
                "for _, row in comparison_df.iterrows():\n",
                "    print(f\"{row['Algorithm']:<25} {row['Hit@1']:<10.4f} {row['Hit@3']:<10.4f} {row['Hit@5']:<10.4f} {row['MRR']:<10.4f}\")\n",
                "\n",
                "# Find best algorithm\n",
                "best_idx = comparison_df['MRR'].idxmax()\n",
                "print(f\"\\nBest Algorithm (by MRR): {comparison_df.loc[best_idx, 'Algorithm']}\")\n",
                "print(\"=\" * 70)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}